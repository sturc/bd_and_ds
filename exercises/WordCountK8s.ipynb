{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# WordCount for K8s"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputFile = \"hdfs://hadoop-hadoop-hdfs-nn:9000/data/ghEmployees.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputFile = \"hdfs://hadoop-hadoop-hdfs-nn:9000/tmp/jwcsturm.txt\""]},{"cell_type":"markdown","metadata":{},"source":["main program"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == \"__main__\":\n","    #create a SparkSession without local master and app name\n","   spark = (SparkSession.builder.getOrCreate())\n","   # read file \n","   spark.sparkContext.setLogLevel(\"ERROR\")\n","   input = spark.sparkContext.textFile(inputFile)\n","   counts = input.flatMap(lambda line : line.split(\" \")).map(lambda word : [word, 1]).reduceByKey(lambda a, b : a + b)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["   # write the result to hdfs\n","   counts.saveAsTextFile(outputFile)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["   print(counts.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["   spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}