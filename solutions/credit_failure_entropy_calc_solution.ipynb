{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Credit Failure Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from scipy.stats import entropy\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from helpers.path_translation import translate_to_file_string\n",
    "\n",
    "inputFile = translate_to_file_string(\"../data/credit_failure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bin_entropy (dataframe, label=\"Kreditausfall\"):\n",
    "    \"\"\" calculates the entropy of the given dataframe based on the given label \"\"\"\n",
    "    numRows= dataframe.count()\n",
    "    truefalse = dataframe.groupBy(label).count()\n",
    "    labelvalues = csv.select(label).dropDuplicates()\n",
    "    if labelvalues.count() != 2 :\n",
    "        raise Exception('infalid datafram or label')\n",
    "    else : \n",
    "        labelval0 = labelvalues.collect()[0][0]\n",
    "        labelval1 = labelvalues.collect()[1][0]\n",
    "\n",
    "        return entropy([truefalse.filter(f\"{label} == '{labelval0}'\").select(\"count\").collect()[0][\"count\"] / numRows, \\\n",
    "                truefalse.filter (f\"{label} == '{labelval1}'\").select(\"count\").collect()[0][\"count\"] / numRows ], base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"Entropy\")\n",
    "             .getOrCreate())\n",
    "\n",
    "csv = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "        .csv(inputFile)\n",
    "csv.show()\n",
    "\n",
    "base_entropy = calc_bin_entropy(csv) \n",
    "print (base_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kopfform_rund_entropy = calc_bin_entropy(csv.filter(\"Kopfform == 'Rund'\"))\n",
    "print (kopfform_rund_entropy)\n",
    "kopfform_quadratisch_entropy = calc_bin_entropy(csv.filter(\"Kopfform == 'Quadratisch'\"))\n",
    "print (kopfform_quadratisch_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koerperform_rechteck = calc_bin_entropy(csv.filter(\"Koerperform == 'Rechteck'\"))\n",
    "print (koerperform_rechteck)\n",
    "koerperform_oval = calc_bin_entropy(csv.filter(\"Koerperform == 'Oval'\"))\n",
    "print (koerperform_oval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koerperfarbe_weiss = calc_bin_entropy(csv.filter(\"Koerperfarbe == 'weiss'\"))\n",
    "print (koerperfarbe_weiss)\n",
    "koerperfarbe_schwarz = calc_bin_entropy(csv.filter(\"Koerperfarbe == 'schwarz'\"))\n",
    "print (koerperfarbe_schwarz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69547edc0321ee61b18014a62d79d67ae43092e495c103c098ab3bf63b872d9b"
  },
  "kernelspec": {
   "display_name": "ds_db-.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
